딥러닝의 시그모이드(Sigmoid)와 소프트맥스(Softmax) 함수에서 밑으로 **자연상수 $e$**를 사용하는 이유는 **수학적 편리성**과 **함수적 특징** 때문이다. 한마디로, **미분하기 가장 편하기 때문**이다.

---

### 1. 🥇 압도적인 미분 편의성

딥러닝 모델은 **경사 하강법(Gradient Descent)**을 이용해 파라미터를 업데이트한다. 이 과정에서 활성화 함수를 미분하여 기울기(Gradient)를 구해야 하는데, 자연상수 $e$를 밑으로 하는 지수함수($e^x$)는 **미분해도 자기 자신이 그대로 나오는 유일한 함수**이다.

* **$e^x$의 미분**:
    $$
    \frac{d}{dx}e^x = e^x
    $$

* **다른 상수 $a$를 밑으로 쓸 경우**:
    $$
    \frac{d}{dx}a^x = a^x \ln(a)
    $$

만약 $e$가 아닌 다른 상수를 사용하면, 위 식처럼 $\ln(a)$라는 불필요한 항이 계속 붙게 된다. 이는 역전파(Backpropagation) 과정에서 계산을 복잡하게 만들고 연산 효율을 떨어뜨린다. $e$를 사용함으로써 기울기 계산이 매우 단순하고 깔끔해진다.

---

### 2. 📈 값의 차이를 증폭시켜 구분 명확화

지수 함수는 입력값의 작은 차이를 출력에서 큰 차이로 **증폭**시키는 특징이 있다.

예를 들어, 모델의 출력 결과가 `[1, 2, 3]`이라고 가정해 보자.

* **그냥 사용하면**: 값의 차이가 크지 않다.
* **$e$를 밑으로 지수 함수를 적용하면**:
    * $e^1 \approx 2.718$
    * $e^2 \approx 7.389$
    * $e^3 \approx 20.086$

입력값의 차이(1)에 비해 출력값의 차이는 훨씬 크게 벌어진다. 소프트맥스 함수는 이렇게 증폭된 값들을 전체 합으로 나누어 확률로 변환하는데, 이 덕분에 모델이 예측하는 가장 유력한 클래스의 확률이 더 **두드러져** 보이게 된다. 즉, 정답을 더 확실하게 예측하도록 유도한다.

---

### 3. ✅ 항상 양수 값을 보장

확률은 음수 값을 가질 수 없다. 지수함수의 출력($e^x$)은 입력값 $x$가 무엇이든 **항상 양수**이다. 따라서 시그모이드나 소프트맥스 함수의 결과를 확률적 개념(0과 1 사이의 값 또는 총합이 1인 확률 분포)으로 해석하는 데 매우 적합하다.

### 결론

> 이론적으로 다른 상수를 사용할 수도 있지만, **미분의 압도적인 간결함**과 **효율적인 학습**을 가능하게 하는 특징 때문에 자연상수 $e$는 딥러닝 분야에서 대체 불가능한 표준으로 자리 잡았다.