{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모멘텀 (Momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개념 요약\n",
    "모멘텀은 경사하강법(Gradient Descent)을 개선하는 최적화 알고리즘 중 하나임. 이전 경사(gradient) 값들을 누적하여 현재 파라미터 업데이트에 반영하는 방식으로, 마치 언덕을 내려오는 공이 관성을 받아 더 빠르고 안정적으로 움직이는 것과 유사함.\n",
    "\n",
    "이러한 관성 덕분에 손실 함수(loss function)의 표면이 완만하거나 지역 최소점(local minima)이 있는 경우에도 더 효과적으로 전역 최소점(global minimum)을 찾아갈 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📐 수학적 배경\n",
    "모멘텀의 업데이트 규칙은 다음과 같이 정의됨. 이 과정은 '속도(velocity)'라는 개념을 도입하여 이전 업데이트의 방향과 크기를 기억함.\n",
    "\n",
    "1. **속도(Velocity) 업데이트:**\n",
    "$$ v_t = \\gamma v_{t-1} + \\eta \\nabla J(W) $$\n",
    "\n",
    "2. **파라미터(Weight) 업데이트:**\n",
    "$$ W = W - v_t $$\n",
    "\n",
    "* $v_t$: 현재 타임스텝(t)에서의 속도 (업데이트할 방향과 크기)\n",
    "* $\\gamma$: 모멘텀 계수(momentum coefficient). 이전 속도를 얼마나 반영할지 결정하는 값 (보통 0.9에 가까운 값 사용)\n",
    "* $v_{t-1}$: 이전 타임스텝(t-1)에서의 속도\n",
    "* $\\eta$: 학습률(learning rate)\n",
    "* $\\nabla J(W)$: 현재 파라미터 $W$에 대한 비용 함수 $J$의 경사(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 임포트 (Setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예제 데이터 준비 (Data Preparation)\n",
    "모멘텀의 효과를 시각적으로 확인하기 위해 간단한 1차원 2차 함수($f(x) = x^2$)를 손실 함수로 가정함. 이 함수의 경사는 $f'(x) = 2x$임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수 정의 (f(x) = x^2)\n",
    "def loss_function(x):\n",
    "    return x**2\n",
    "\n",
    "# 손실 함수의 경사(미분) 정의 (f'(x) = 2x)\n",
    "def gradient(x):\n",
    "    return 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 핵심 로직 구현 (Code Implementation)\n",
    "표준 경사하강법과 모멘텀을 적용한 경사하강법을 각각 구현하여 파라미터가 어떻게 업데이트되는지 비교함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "learning_rate = 0.1 # 학습률\n",
    "gamma = 0.9         # 모멘텀 계수\n",
    "n_iterations = 20   # 반복 횟수\n",
    "start_x = 10.0      # 시작 파라미터 값\n",
    "\n",
    "# 결과를 저장할 리스트 초기화\n",
    "gd_history = []\n",
    "momentum_history = []\n",
    "\n",
    "### 1. 표준 경사하강법 (Standard Gradient Descent)\n",
    "x_gd = start_x\n",
    "for i in range(n_iterations):\n",
    "    gd_history.append(x_gd)\n",
    "    grad = gradient(x_gd) # 경사 계산\n",
    "    x_gd = x_gd - learning_rate * grad # 파라미터 업데이트\n",
    "\n",
    "### 2. 모멘텀 (Momentum)\n",
    "x_momentum = start_x\n",
    "velocity = 0.0 # 속도 초기화\n",
    "\n",
    "print(\"--- 모멘텀 최적화 과정 ---\")\n",
    "for i in range(n_iterations):\n",
    "    momentum_history.append(x_momentum)\n",
    "    grad = gradient(x_momentum)\n",
    "    \n",
    "    # 이전 속도와 현재 경사를 사용하여 새로운 속도를 계산\n",
    "    # 이것이 모멘텀의 핵심 아이디어임\n",
    "    velocity = gamma * velocity + learning_rate * grad\n",
    "    \n",
    "    # 새로운 속도를 사용하여 파라미터 업데이트\n",
    "    x_momentum = x_momentum - velocity\n",
    "\n",
    "    if i % 2 == 0: # 2번의 반복마다 중간 과정 출력\n",
    "        print(f\"Iter {i+1:2d}: Grad={grad:8.4f}, Velocity={velocity:8.4f}, New_X={x_momentum:8.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📊 결과 확인 및 시각화 (Results & Visualization)\n",
    "두 최적화 방법의 파라미터 업데이트 경로를 시각화하여 비교함. 모멘텀이 더 적은 반복으로, 더 부드럽게 최적점에 수렴하는 것을 확인할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수 그래프 생성\n",
    "x_vals = np.linspace(-12, 12, 100)\n",
    "y_vals = loss_function(x_vals)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(x_vals, y_vals, label='Loss Function ($x^2$)', color='gray')\n",
    "\n",
    "# 표준 경사하강법 경로 시각화\n",
    "plt.plot(gd_history, loss_function(np.array(gd_history)), 'o--', label='Standard Gradient Descent', markersize=5)\n",
    "\n",
    "# 모멘텀 경로 시각화\n",
    "plt.plot(momentum_history, loss_function(np.array(momentum_history)), 'o-', label='Momentum', markersize=8)\n",
    "\n",
    "plt.title('Gradient Descent vs Momentum', fontsize=16)\n",
    "plt.xlabel('Parameter (x)')\n",
    "plt.ylabel('Loss f(x)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 고려사항 (Considerations)\n",
    "\n",
    "* <b>모멘텀 계수($\\gamma$)의 역할</b>: `gamma` 값이 클수록(예: 0.99) 이전 업데이트의 영향을 더 많이 받아 관성이 커짐. 이로 인해 최적점을 지나칠 수도 있음. 반면, 값이 작으면(예: 0.5) 표준 경사하강법과 유사하게 동작함. \n",
    "* **Nesterov Accelerated Gradient (NAG)**: 모멘텀을 개선한 버전으로, 현재 속도로 한 걸음 먼저 이동한 위치에서 경사를 계산함. 불필요한 움직임을 줄여 더 효율적으로 수렴하는 경향이 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 요약 (Conclusion)\n",
    "모멘텀은 이전 업데이트 단계를 관성처럼 활용하여 최적화 과정의 속도와 안정성을 높이는 기법임. 손실 함수의 경사가 작아 학습이 느려지는 구간에서는 속도를 높여주고, 경사 방향이 계속 바뀌는 구간에서는 이전 방향을 유지하여 진동을 줄여줌.\n",
    "\n",
    "예제를 통해 표준 경사하강법보다 모멘텀이 더 빠르고 부드럽게 최적점에 도달하는 것을 시각적으로 확인했음. 이는 복잡한 딥러닝 모델 학습에서 매우 중요한 역할을 함."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}