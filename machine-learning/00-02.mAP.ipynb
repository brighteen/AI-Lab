{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 객체 탐지 성능 지표: mAP50 vs mAP50-95 이해하기\n",
    "\n",
    "객체 탐지 모델의 성능은 mAP(mean Average Precision)라는 종합적인 지표로 평가.\n",
    "이 노트북은 mAP 계산의 기반이 되는 IoU, AP 개념을 먼저 알아보고, 가장 널리 쓰이는 mAP50과 mAP50-95의 차이를 코드로 직접 확인하는 것을 목표로 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 기본 설정 및 유틸리티 함수\n",
    "\n",
    "시각화 및 계산에 필요한 라이브러리를 임포트하고, 경계 상자를 그리는 유틸리티 함수를 정의."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def plot_boxes(box_a, box_b, title):\n",
    "    \"\"\"두 개의 경계 상자와 IoU를 시각화하는 함수\"\"\"\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # box_a (정답 상자) 그리기\n",
    "    rect_a = patches.Rectangle((box_a[0], box_a[1]), box_a[2]-box_a[0], box_a[3]-box_a[1], linewidth=2, edgecolor='r', facecolor='none', label='Ground Truth')\n",
    "    ax.add_patch(rect_a)\n",
    "\n",
    "    # box_b (예측 상자) 그리기\n",
    "    rect_b = patches.Rectangle((box_b[0], box_b[1]), box_b[2]-box_b[0], box_b[3]-box_b[1], linewidth=2, edgecolor='b', facecolor='none', label='Prediction')\n",
    "    ax.add_patch(rect_b)\n",
    "    \n",
    "    iou = calculate_iou(box_a, box_b)\n",
    "    plt.title(f'{title}\\nIoU: {iou:.4f}')\n",
    "    plt.xlim(0, 12)\n",
    "    plt.ylim(0, 12)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. IoU (Intersection over Union) 계산\n",
    "\n",
    "IoU는 모델이 예측한 경계 상자(Bounding Box)와 실제 정답 상자가 얼마나 겹치는지를 나타내는 지표. 모델 예측의 '성공' 여부를 판단하는 첫 단계.\n",
    "\n",
    "- **계산식**: (교집합 영역 넓이) / (합집합 영역 넓이)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box_a, box_b):\n",
    "    \"\"\"두 경계 상자(x1, y1, x2, y2)의 IoU를 계산하는 함수\"\"\"\n",
    "    # 교집합(intersection) 영역 좌표 계산\n",
    "    x_a = max(box_a[0], box_b[0])\n",
    "    y_a = max(box_a[1], box_b[1])\n",
    "    x_b = min(box_a[2], box_b[2])\n",
    "    y_b = min(box_a[3], box_b[3])\n",
    "\n",
    "    # 교집합 넓이 계산\n",
    "    intersection_area = max(0, x_b - x_a) * max(0, y_b - y_a)\n",
    "\n",
    "    # 각 상자의 넓이 계산\n",
    "    box_a_area = (box_a[2] - box_a[0]) * (box_a[3] - box_a[1])\n",
    "    box_b_area = (box_b[2] - box_b[0]) * (box_b[3] - box_b[1])\n",
    "\n",
    "    # 합집합(union) 넓이 계산\n",
    "    union_area = box_a_area + box_b_area - intersection_area\n",
    "\n",
    "    # IoU 계산\n",
    "    iou = intersection_area / union_area if union_area > 0 else 0\n",
    "    return iou\n",
    "\n",
    "# 예시 데이터: [x1, y1, x2, y2]\n",
    "ground_truth_box = [2, 2, 8, 8]  # 정답 상자\n",
    "\n",
    "# Case 1: 예측이 매우 정확한 경우\n",
    "good_prediction_box = [2.5, 2.5, 8.5, 8.5]\n",
    "plot_boxes(ground_truth_box, good_prediction_box, 'Case 1: Good Prediction')\n",
    "\n",
    "# Case 2: 예측이 부정확한 경우\n",
    "bad_prediction_box = [5, 5, 10, 10]\n",
    "plot_boxes(ground_truth_box, bad_prediction_box, 'Case 2: Bad Prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AP (Average Precision) 계산\n",
    "AP는 한 클래스에 대한 모델의 종합 성능. 정밀도(Precision)와 재현율(Recall)을 모두 고려하여 계산.\n",
    "\n",
    "1.  **IoU 임계값**을 기준으로 예측들을 **TP(True Positive)** 또는 **FP(False Positive)**로 분류.\n",
    "2.  예측들을 **신뢰도(Confidence Score)** 순으로 정렬.\n",
    "3.  정렬된 순서대로 정밀도-재현율 곡선을 그려 아래 면적(AP)을 계산."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ap(gts, preds, iou_threshold):\n",
    "    \"\"\"단일 클래스에 대한 AP를 계산하는 함수\"\"\"\n",
    "    # 예측을 신뢰도 점수 기준으로 내림차순 정렬\n",
    "    preds = sorted(preds, key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    tps = np.zeros(len(preds))\n",
    "    fps = np.zeros(len(preds))\n",
    "    total_true_positives = len(gts)\n",
    "    \n",
    "    # 각 예측에 대해 TP/FP 판정\n",
    "    detected_gts = [] # 이미 매칭된 정답 상자\n",
    "    for i, pred in enumerate(preds):\n",
    "        best_iou = 0\n",
    "        best_gt_idx = -1\n",
    "        for j, gt in enumerate(gts):\n",
    "            iou = calculate_iou(pred['box'], gt['box'])\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_gt_idx = j\n",
    "        \n",
    "        if best_iou >= iou_threshold and best_gt_idx not in detected_gts:\n",
    "            tps[i] = 1\n",
    "            detected_gts.append(best_gt_idx)\n",
    "        else:\n",
    "            fps[i] = 1\n",
    "            \n",
    "    # 정밀도 및 재현율 계산\n",
    "    cumulative_tps = np.cumsum(tps)\n",
    "    cumulative_fps = np.cumsum(fps)\n",
    "    \n",
    "    recalls = cumulative_tps / total_true_positives if total_true_positives > 0 else 0\n",
    "    precisions = cumulative_tps / (cumulative_tps + cumulative_fps)\n",
    "    \n",
    "    # AP 계산 (11-point interpolation)\n",
    "    ap = 0\n",
    "    for t in np.arange(0., 1.1, 0.1):\n",
    "        if np.sum(recalls >= t) == 0:\n",
    "            p = 0\n",
    "        else:\n",
    "            p = np.max(precisions[recalls >= t])\n",
    "        ap += p / 11\n",
    "        \n",
    "    return ap, recalls, precisions\n",
    "\n",
    "# 예시 데이터 (단일 클래스: 'cat')\n",
    "gts_cat = [{'box': [1, 2, 4, 5]}, {'box': [6, 6, 9, 9]}]\n",
    "preds_cat = [\n",
    "    {'box': [1.1, 2.1, 4.2, 5.2], 'confidence': 0.95}, # IoU 높음, TP 예상\n",
    "    {'box': [5, 5, 8, 8], 'confidence': 0.88},       # IoU 낮음, FP 예상\n",
    "    {'box': [6.1, 6.2, 9.1, 9.2], 'confidence': 0.98}  # IoU 높음, TP 예상\n",
    "]\n",
    "\n",
    "# IoU 임계값 0.5에서 AP 계산\n",
    "ap_50, recalls, precisions = calculate_ap(gts_cat, preds_cat, iou_threshold=0.5)\n",
    "print(f\"IoU Threshold: 0.5\")\n",
    "print(f\"AP: {ap_50:.4f}\")\n",
    "\n",
    "# IoU 임계값 0.8에서 AP 계산 (더 엄격한 기준)\n",
    "ap_80, _, _ = calculate_ap(gts_cat, preds_cat, iou_threshold=0.8)\n",
    "print(f\"\\nIoU Threshold: 0.8\")\n",
    "print(f\"AP: {ap_80:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. mAP50 vs mAP50-95 계산 및 비교\n",
    "\n",
    "이제 핵심 개념인 mAP50과 mAP50-95를 계산. 둘의 차이는 **어떤 IoU 임계값을 사용해 AP를 계산하고 평균 내는가**에 있음.\n",
    "\n",
    "- **mAP50**: IoU 임계값을 **0.5 하나로 고정**하고 mAP 계산. 위치가 다소 부정확해도 너그럽게 평가.\n",
    "- **mAP50-95**: IoU 임계값을 **0.5부터 0.95까지 0.05씩** 바꿔가며 각각 mAP를 계산한 후, 그 결과들을 다시 **평균**낸 값. 위치 정확도까지 엄격하게 평가."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 클래스를 가정한 예시 데이터\n",
    "# gts: {'class_name': [gt_boxes, ...]}\n",
    "# preds: {'class_name': [pred_boxes, ...]}\n",
    "all_gts = {\n",
    "    'cat': [{'box': [1, 2, 4, 5]}, {'box': [6, 6, 9, 9]}],\n",
    "    'dog': [{'box': [3, 3, 6, 7]}]\n",
    "}\n",
    "\n",
    "all_preds = {\n",
    "    'cat': [\n",
    "        {'box': [1.1, 2.1, 4.2, 5.2], 'confidence': 0.95}, # cat GT 1과 IoU 높음\n",
    "        {'box': [5, 5, 8, 8], 'confidence': 0.88},       # 모든 cat GT와 IoU 낮음\n",
    "        {'box': [6.1, 6.2, 9.1, 9.2], 'confidence': 0.98}  # cat GT 2와 IoU 높음\n",
    "    ],\n",
    "    'dog': [\n",
    "        {'box': [3.5, 3.5, 6.5, 7.5], 'confidence': 0.92} # dog GT와 IoU 중간 정도\n",
    "    ]\n",
    "}\n",
    "\n",
    "classes = all_gts.keys()\n",
    "\n",
    "# --- mAP50 계산 ---\n",
    "print(\"--- Calculating mAP50 ---\")\n",
    "ap_per_class_50 = []\n",
    "for cls in classes:\n",
    "    ap, _, _ = calculate_ap(all_gts[cls], all_preds.get(cls, []), iou_threshold=0.5)\n",
    "    ap_per_class_50.append(ap)\n",
    "    print(f\"  AP@.50 for class '{cls}': {ap:.4f}\")\n",
    "\n",
    "mAP50 = np.mean(ap_per_class_50)\n",
    "print(f\"\\nResult -> mAP50: {mAP50:.4f}\\n\")\n",
    "\n",
    "# --- mAP50-95 계산 ---\n",
    "print(\"--- Calculating mAP50-95 ---\")\n",
    "iou_thresholds = np.arange(0.5, 1.0, 0.05)\n",
    "mAPs = []\n",
    "\n",
    "for iou_t in iou_thresholds:\n",
    "    ap_per_class = []\n",
    "    for cls in classes:\n",
    "        ap, _, _ = calculate_ap(all_gts[cls], all_preds.get(cls, []), iou_threshold=iou_t)\n",
    "        ap_per_class.append(ap)\n",
    "    \n",
    "    mAP_at_t = np.mean(ap_per_class)\n",
    "    mAPs.append(mAP_at_t)\n",
    "    print(f\"  mAP @ IoU={iou_t:.2f}: {mAP_at_t:.4f}\")\n",
    "\n",
    "mAP50_95 = np.mean(mAPs)\n",
    "print(f\"\\nResult -> mAP50-95: {mAP50_95:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과 분석\n",
    "\n",
    "위 코드 실행 결과를 보면, IoU 임계값이 높아질수록(기준이 엄격해질수록) mAP 점수가 하락하는 경향을 보임. \n",
    "\n",
    "- `dog` 클래스의 예측은 IoU가 0.5 근처였기 때문에, mAP50 계산 시에는 높은 AP를 기록했지만, 임계값이 0.7 이상으로 올라가자 AP가 0으로 급격히 떨어짐.\n",
    "- 이로 인해 최종 `mAP50-95` 점수는 `mAP50` 점수보다 훨씬 낮게 계산됨.\n",
    "- 이는 `mAP50-95`가 객체 탐지 능력뿐만 아니라, **위치 예측의 정밀함(Localization Accuracy)**까지 종합적으로 평가하는 지표임을 명확히 보여줌."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 최종 요약\n",
    "\n",
    "mAP50은 모델이 객체를 '찾았는가'를, mAP50-95는 '얼마나 정확한 위치에 찾았는가'까지 종합적으로 평가하는 지표이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}